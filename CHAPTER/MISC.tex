\chapter{Miscellaneous Topics}
In this chapter, we discuss some miscellaneous topics that are relevant to numerical implemenntations of consitutive models.
\section{Equivalent But Preferred?}
Some (yield) functions are mathematically equivalent, is there a preferred one?
Or any form can be used?
Consider for example, we want to define a yield function that enforces the magnitude of stress $\sigma$ to be less than a yield stress $\sigma_y\geqslant0$, one can write
\begin{gather}
    f=\abs{\sigma}-\sigma_y,
\end{gather}
or alternatively, one can write
\begin{gather}
    f=\sigma^2-\sigma_y^2.
\end{gather}
They are indeed mathematically equivalent as we have defined the yield stress to be non-negative.
However, they may have different implications in terms of numerical implementation.
First, the involvement of squares is discouraged.
It easily leads to precision issues, especially when the local nonlinear system also contains some nondimentionsal history variables.
Second, if associative flow is used, the above two yield functions lead to
\begin{gather}
    \dot{\varepsilon^p}=\gamma\pdfrac{f}{\sigma}=\left\{\begin{array}{ll}
        \gamma\sign{\sigma}, & \text{for~}f=\abs{\sigma}-\sigma_y, \\[4mm]
        \gamma2\sigma,       & \text{for~}f=\sigma^2-\sigma_y^2.
    \end{array}\right.
\end{gather}
It is easy to see that in the first case, $\abs{\dot{\varepsilon^p}}=\gamma$, meaning that $\gamma<\abs{\dot{\varepsilon}}$ is bounded as the increment of plastic strain can never exceed the increment of total strain.
However, in the second case, it is not easy to determine the upper bound of $\gamma$.
Since the whole plasticity development is driven by the plastic multiplier $\gamma$, it is possible to use root finding algorithms to solve for $\gamma$ when an explicit bound can be found.
This is particularly useful when the local Newton--Raphson iteration fails to converge.

For numerical robustness, it is preferred to use the first form of yield function in this case.
Similar arguments can be made for other more complex formulations.

Can you analyze the pros and cons of the third option shown below?
\begin{gather}
    f=\dfrac{\abs{\sigma}}{\sigma_y}-1.
\end{gather}
\section{First Order or Second Order?}
Let's consider the following ODE,
\begin{gather}
    \dot{y}=a-by,
\end{gather}
with the initial condition $y(0)=0$.
The analytical solution can be easily obtained as
\begin{gather}
    y(t)=\dfrac{a}{b}\left(1-e^{-bt}\right).
\end{gather}

Now, lest consider integrating this ODE using the first--order backward Euler method and the second--order Trapezoidal method.
\begin{figure}[ht]
    \centering
    \includegraphics{PIC/convergence.integration.pdf}
    \caption{comparison of first--order backward Euler method and second--order Trapezoidal method for the ODE $\dot{y}=a-by$.}
    \label{fig:convergence}
\end{figure}
\figref{fig:convergence} presents the error of the numerical solutions for $a=1$ and $b=20$.
The system is integrated to $t=10$.
As expected, when $\Delta{}t$ decreases, the Trapezoidal method shows a second--order convergence, while the backward Euler method shows a first--order convergence.
This is well understood via the truncation error analysis.

However, when $\Delta{}t$ is large, the absolute error of the Euler method is smaller in the above example.
Even if both methods show perfect first--order and second--order convergence for arbitrary $\Delta{}t$, there would exist a threshold such that when $\Delta{}t$ is larger than the threshold, the first--order method would yield a smaller absolute error than the second--order method.
After all, the term `second--order' only tells us that the error is $\mathcal{O}\left(\Delta{}t^2\right)$, it does not imply that for a given $\Delta{}t$, second--order methods will always yield a smaller absolute error than first--order methods.
The constant factor dropped in the big-$\mathcal{O}$ notation can be large enough to make the second--order method worse than the first--order method for a given $\Delta{}t$.

This is not surprising --- two straight lines with different slopes will eventually cross each other.
The question is, is this `threshold' practically relevant?
There does not exist a universal answer to this question.
One shall thus be aware of this subtle implication.
When designing a numerical implementation, a second--order method is not necessarily universally better than a first--order method.
\section{Algorithmic Tangent Stiffness}
In numerical implementations with implicit schemes, the algorithmic tangent stiffness is often required to ensure the quadratic convergence of the global Newton--Raphson iteration.
It is also called the consistent tangent operator in some literature.

But what is it exactly?
Let's consider the following ODE as an example,
\begin{gather}
    \ddfrac{y}{x}=y.
\end{gather}
For a function whose derivative is the function itself, we know the solution is the exponential function,
\begin{gather}
    y=e^x+C
\end{gather}
where $C$ is some constant.

Now, pretending that we do not know the analytical solution, we want to solve this ODE numerically using the backward Euler method.
We have
\begin{gather}
    y_{n+1}-y_n=y\Delta{}x,
\end{gather}
rearranging the equation gives
\begin{gather}
    y_{n+1}=\dfrac{y_n}{1-\Delta{}x}.
\end{gather}
This shall be our recursion formula.
Omit the subscript $\left(\cdot\right)_{n+1}$ for simplicity and compute the derivative of $y$ with respect to $\Delta{}x$, one has
\begin{gather}
    \ddfrac{y}{\Delta{}x}=\dfrac{y_n}{\left(1-\Delta{}x\right)^2}=\dfrac{y}{1-\Delta{}x}.
\end{gather}
On the other hand, because $x=x_n+\Delta{}x$, we have
\begin{gather}
    \md{x}=\md{x_n+\Delta{}x}=\md{\Delta{}x},
\end{gather}
the above derivative is thus
\begin{gather}
    \ddfrac{y}{x}=\dfrac{y}{1-\Delta{}x}.
\end{gather}
It is clear that this is not the same as the original ODE $\ddfrac{y}{x}=y$.
However, when $\Delta{}x\to0$, the derivative obtained from discretized equation converges to the original analytical one.

This is the essence of the algorithmic tangent stiffness.
It is the derivative obtained from the discretized equation, which is \textit{consistent} with the numerical scheme used.
